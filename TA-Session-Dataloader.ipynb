{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. arguments in dataloader\n",
    "\n",
    "dataloader의 arguments는 다음과 같이 구성됩니다.\n",
    "\n",
    "```python\n",
    "DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None)\n",
    "```\n",
    "\n",
    "여기에서는 \n",
    "\n",
    "* dataset\n",
    "* num_workers and pin_memory\n",
    "* collate_fn\n",
    "\n",
    "에 대해서 알아봅니다.\n",
    "\n",
    "다른 사항 혹은 더 자세한 내용은 [여기](https://pytorch.org/docs/stable/data.html)를 참고해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. map-style dataset\n",
    "\n",
    "dataset에는\n",
    "\n",
    "* map-style과\n",
    "* iterable-style의 dataset이 존재합니다.\n",
    "\n",
    "Map-style dataset은 dataset class안에 ```__getitem__``` 함수와 ```__len__``` 함수가 존재합니다. (이를 python slice obejct라고 합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class map_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.data = np.arange(5.)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # slicing rule with index i\n",
    "        d = self.data[i]\n",
    "        return  d\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "m_dataset = map_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에서 ```__getitem__``` 함수는 다음과 같이 두 가지 방식으로 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "print(m_dataset.__getitem__(2))\n",
    "print(m_dataset[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마찬가지로 ```__len__``` 함수는 다음과 같이 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(m_dataset.__len__())\n",
    "print(len(m_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 우리는 기존의 python slicing에 사용하던 다음 문법을 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 2.]\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "print(m_dataset[0:4:2])\n",
    "print(m_dataset[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. iterable-style dataset\n",
    "\n",
    "Iterable-style dataset은 dataset class안에 ```__iter__``` 함수가 존재합니다. (이를 python iterable object라고 합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "class iter_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.data = np.arange(5.)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return None\n",
    "    \n",
    "i_dataset = iter_dataset()\n",
    "from collections.abc import Iterable\n",
    "print(isinstance(i_dataset, Iterable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML에서는 보통 iterable 객체가 아니라 여기에 ```next``` 함수를 적용할 수 있는 iterator를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "class iter_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.data = np.arange(5.)\n",
    "        self.i = 0\n",
    "        self.n = len(self.data)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        self.i += 1\n",
    "        if self.i < self.n:\n",
    "            return self.data[self.i]\n",
    "        else:\n",
    "            raise StopIteration()\n",
    "            \n",
    "    # this function is optional!\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "i_dataset = iter_dataset()\n",
    "iter(i_dataset)\n",
    "print(next(i_dataset))\n",
    "print(next(i_dataset))\n",
    "print(next(i_dataset))\n",
    "print(next(i_dataset))\n",
    "\n",
    "# print(next(i_dataset))\n",
    "# IndexError: index 5 is out of bounds for axis 0 with size 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 정의된 iterator의 전형적인 사용 방식은 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "0 1.0\n",
      "1 2.0\n",
      "2 3.0\n",
      "3 4.0\n"
     ]
    }
   ],
   "source": [
    "for d in iter_dataset():\n",
    "    print(d)\n",
    "\n",
    "for i, d in enumerate(iter_dataset()):\n",
    "    print(i, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한편 ```__len__```이 구현되어 있는 함수의 경우에는 progress bar를 다음과 같이 볼 수 있습니다.\n",
    "\n",
    "이때 progress bar와 함께 볼 수 있는 it/s 정보는 num_workers를 결정할때 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d14d2ed77e54321bcf5eef217fb5833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 13923.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0\n",
      "1 2.0\n",
      "2 3.0\n",
      "3 4.0\n",
      "\n",
      "0 1.0\n",
      "1 2.0\n",
      "2 3.0\n",
      "3 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "for i,d in tqdm_notebook(enumerate(iter_dataset())):\n",
    "    print(i,d)\n",
    "    \n",
    "from tqdm import tqdm\n",
    "for i,d in tqdm(enumerate(iter_dataset())):\n",
    "    print(i,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 길이가 정해져 있지 않는 경우에도 다음과 같이 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "class iter_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.i = 0\n",
    "    \n",
    "    # this function is a generator\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            self.i += 1\n",
    "            yield self.i\n",
    "    \n",
    "i_dataset = iter(iter_dataset())\n",
    "print(next(i_dataset))\n",
    "print(next(i_dataset))\n",
    "print(next(i_dataset))\n",
    "print(next(i_dataset))\n",
    "print(next(i_dataset))\n",
    "print(next(i_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 Dataloader가 argument로 받을 수 있었던 sampler는 ```next``` 함수가 없는 map-style dataset에서 data loading 순서를 정하고 싶을때 유용하게 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. num_workers\n",
    "\n",
    "Dataloader argument에서 default num_workers 값은 0으로 이는 main process를 사용해 data loading을 한다는 뜻입니다.\n",
    "\n",
    "그밖에도 양의 정수값을 num_workers로 설정할 수 있지만, 어떤 값이 가장 좋은지는 \n",
    "\n",
    "* 알고리즘, \n",
    "* 데이터, \n",
    "* 구동 사양에 \n",
    "\n",
    "따라 다르기 때문에 여러값을 해보시는 것을 추천드립니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. pin_memory\n",
    "\n",
    "```pin_memory``` 함수는 cpu에 있는 tensor를 받아서 gpu에 빠르게 올릴 수 있도록 data를 저장합니다. \n",
    "\n",
    "Dataloader에 있는 argument인 pin_memory는 이 pin_memory 함수를 data loading하는 과정에서 자동으로 부를 지를 선택할 수 있습니다.\n",
    "\n",
    "한 가지 유의해야할 점은 이렇게 제공되는 pin_memory argument는 mini batch가 tensor 혹은 tensor를 포함한 iterable일때만 제대로 작동한다는 점입니다. 그렇지 않은 경우에는 [여기](https://pytorch.org/docs/stable/data.html#memory-pinning)를 참고해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. collate_fn\n",
    "\n",
    "```collate_fn``` 의 default값은 ```None```으로 이 경우에 ```collate_fn```이 하는 역할은\n",
    "\n",
    "* ```(data, label)```의 tuple의 list로 구성되어 있는 mini-batch를 ```(mini-batch data, mini-batc label)```의 single tuple로 바꿔줍니다.(따라서 mini-batch data는 기존의 data보다 차원이 하나 더 추가됩니다.)\n",
    "* 이 과정에서 numerical value (python, numpy)들을 tensor로 바꿔줍니다.\n",
    "\n",
    "입니다.\n",
    "\n",
    "한편, 이 과정을 customize하는 것 또한 가능한데 이를 위해서는 직접 ```list```를 받아서 비슷한 역할을 해줄 수 있는 함수를 작성해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic dataloader:\n",
      "tensor([34.,  0., 15., 16.,  2., 24.,  3., 30., 21., 19.], dtype=torch.float64)\n",
      "tensor([27.,  6., 48., 38., 35., 10., 45., 39.,  8., 11.], dtype=torch.float64)\n",
      "tensor([33., 22., 42.,  4., 37., 13.,  7., 49., 41., 12.], dtype=torch.float64)\n",
      "tensor([46., 32., 40., 31., 47., 25., 14., 17.,  1., 43.], dtype=torch.float64)\n",
      "tensor([26., 20.,  5., 29., 28., 44., 18., 36., 23.,  9.], dtype=torch.float64)\n",
      "Dataloader with negative_fn:\n",
      "[-3.0, -21.0, -33.0, -6.0, -16.0, -37.0, -27.0, -29.0, -5.0, -19.0]\n",
      "[-32.0, -12.0, -47.0, -4.0, -30.0, -28.0, -24.0, -11.0, -43.0, -1.0]\n",
      "[-20.0, -9.0, -15.0, -42.0, -2.0, -39.0, -48.0, -38.0, -44.0, -41.0]\n",
      "[-10.0, -40.0, -46.0, -35.0, -17.0, -22.0, -0.0, -45.0, -36.0, -34.0]\n",
      "[-7.0, -8.0, -14.0, -23.0, -26.0, -25.0, -18.0, -13.0, -49.0, -31.0]\n"
     ]
    }
   ],
   "source": [
    "# thanks to Juhyuk Lee (https://github.com/sehkmg)\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class map_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.data = np.arange(50.)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # slicing rule with index i\n",
    "        d = self.data[i]\n",
    "        return  d\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "m_dataset = map_dataset()\n",
    "\n",
    "def negative_fn(batch):\n",
    "    batch = [-b for b in batch]\n",
    "#     batch = torch.Tensor(batch)\n",
    "    return batch\n",
    "\n",
    "m_dataloader = DataLoader(m_dataset, batch_size=10, shuffle=True)\n",
    "m_dataloader_neg = DataLoader(m_dataset, batch_size=10, shuffle=True, collate_fn=negative_fn)\n",
    "\n",
    "print('Basic dataloader:')\n",
    "for batch in m_dataloader:\n",
    "    print(batch)\n",
    "\n",
    "print('Dataloader with negative_fn:')\n",
    "for batch in m_dataloader_neg:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. random seed fix\n",
    "\n",
    "실험의 reproducibility를 위해서는 pseudo random number generator의 seed를 고정하여 randomness가 개입되는 상황 (mini-batch sampling, random sampling with fixed distribution)에서 같은 숫자가 sample되도록 하는 것이 중요합니다. 아래의 함수는 이와 같은 상황에서 유용합니다. (하지만 주석의 링크에서 확인할 수 있듯이, 이렇게 해도 완전한 reproducing을 보장하지는 못합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    import os\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    # for reproducibility. \n",
    "    # note that pytorch is not completely reproducible \n",
    "    # https://pytorch.org/docs/stable/notes/randomness.html  \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.initial_seed() # dataloader multi processing \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
